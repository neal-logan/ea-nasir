{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 01 Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Designed for Python 3.11.9 on Windows 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install from requirements.txt\n",
    "!python -m pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative for running in Colab etc\n",
    "!pip install --upgrade pip\n",
    "!pip install numpy==1.24.1\n",
    "!pip install pmdarima==2.0.4\n",
    "!pip install ibis-framework[duckdb]==9.5.0\n",
    "!pip install pandas==2.2.3\n",
    "!pip install matplotlib==3.9.2\n",
    "!pip install skforecast==0.14.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Key tools:\n",
    "- [Ibis](https://ibis-project.org/) on a [DuckDB](https://duckdb.org/) backend with transformations to [Pandas](https://pandas.pydata.org/docs/index.html) for certain operations.\n",
    "- Some excessively-verbose functions spun out to a data prep module "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import ibis\n",
    "import matplotlib.pyplot as plt\n",
    "import skforecast\n",
    "import pmdarima\n",
    "import methods.prep as prep\n",
    "import methods.vis as vis\n",
    "import importlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(vis)\n",
    "importlib.reload(prep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load Individual Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con = ibis.connect(\"duckdb://\")\n",
    "data_path = 'data_staged/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load FRED datasets\n",
    "fred_dataset_names = [\n",
    "    'consumer-sentiment',\n",
    "    'cpi',\n",
    "    'hourly-wage',\n",
    "    'house-median-price',\n",
    "    'house-starts',\n",
    "    'unemployment',\n",
    "]\n",
    "\n",
    "fred_datasets = {}\n",
    "\n",
    "print('\\nFRED Datasets (single-variable time series)')\n",
    "for name in fred_dataset_names:\n",
    "    fred_datasets[name] = ibis.read_csv(\n",
    "        data_path + name + '.csv',\n",
    "        dateformat='%m/%d/%Y')\n",
    "    print(name + ': ' + str(fred_datasets[name].to_pandas().shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load investing.com datasets\n",
    "\n",
    "inv_dataset_names = [\n",
    "    'copper', \n",
    "    'corn', \n",
    "    'gold',\n",
    "    'lumber',\n",
    "    'natural-gas',\n",
    "    'oil-wti',\n",
    "    'r2000', \n",
    "    'soy',\n",
    "    'sp500',\n",
    "    'vix-volatility'\n",
    "]\n",
    "\n",
    "traded_commodities = [\n",
    "    'copper'\n",
    "]\n",
    "\n",
    "inv_datasets = {}\n",
    "\n",
    "print('Investing.com Datasets (standard prices and volume time series)')\n",
    "for name in inv_dataset_names:\n",
    "    inv_datasets[name] = ibis.read_csv(\n",
    "        data_path + name + '.csv',\n",
    "        dateformat='%m/%d/%Y')\n",
    "    print(name + ': ' + str(inv_datasets[name].to_pandas().shape))\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Initial Preprocessing - Individual Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in inv_datasets.keys():\n",
    "    #Drop unnecessary columns\n",
    "    inv_datasets[key] = inv_datasets[key].drop('Vol.','Change %')\n",
    "\n",
    "    #Add trading day flag to traded columns\n",
    "    if key in traded_commodities:\n",
    "        inv_datasets[key] = inv_datasets[key].mutate(\n",
    "            TRADING_DAY = True\n",
    "        )\n",
    "\n",
    "    #Convert column names to ALL CAPS\n",
    "    inv_datasets[key] = inv_datasets[key].rename('ALL_CAPS')\n",
    "\n",
    "    # Convert PRICE, OPEN, HIGH, and LOW to float64s if they are strings\n",
    "    if inv_datasets[key]['PRICE'].type().is_string():\n",
    "        inv_datasets[key] = inv_datasets[key].mutate(\n",
    "            PRICE = inv_datasets[key]['PRICE'].replace(',','').cast('float64'))    \n",
    "\n",
    "    if inv_datasets[key]['OPEN'].type().is_string():\n",
    "        inv_datasets[key] = inv_datasets[key].mutate(\n",
    "            OPEN = inv_datasets[key]['OPEN'].replace(',','').cast('float64'))   \n",
    "\n",
    "    if inv_datasets[key]['HIGH'].type().is_string():\n",
    "        inv_datasets[key] = inv_datasets[key].mutate(\n",
    "            HIGH = inv_datasets[key]['HIGH'].replace(',','').cast('float64'))  \n",
    "\n",
    "    if inv_datasets[key]['LOW'].type().is_string():\n",
    "        inv_datasets[key] = inv_datasets[key].mutate(\n",
    "            LOW = inv_datasets[key]['LOW'].replace(',','').cast('float64'))   \n",
    "\n",
    "    # Add prefixes to each column based on the name of the dataset\n",
    "    names_map = {\n",
    "        f\"{key}_{col}\" : col \n",
    "            for col in inv_datasets[key].columns\n",
    "            if col != 'DATE'}\n",
    "    inv_datasets[key] = inv_datasets[key].rename(names_map)\n",
    "\n",
    "for k,v in inv_datasets.items():\n",
    "    print(v.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix column names in FRED data and drop unused rows\n",
    "for key in fred_datasets.keys():\n",
    "    #Rename second column (data) to name of dataset\n",
    "    old_col_name = fred_datasets[key].columns[1]\n",
    "    new_col_name = key\n",
    "    fred_datasets[key] = fred_datasets[key].rename({new_col_name: old_col_name})\n",
    "\n",
    "    #Convert timestamps to dates\n",
    "    fred_datasets[key] = fred_datasets[key].mutate(\n",
    "        DATE = fred_datasets[key]['DATE'].cast('date'))\n",
    "\n",
    "    #Convert to snake-case\n",
    "    fred_datasets[key] = fred_datasets[key].rename('ALL_CAPS')\n",
    "\n",
    "    #Drop unused rows \n",
    "    fred_datasets[key] = fred_datasets[key].filter(fred_datasets[key].DATE.year() > 2005)\n",
    "\n",
    "# Convert string to float64 in consumer sentiment data   \n",
    "fred_datasets['consumer-sentiment'] = fred_datasets['consumer-sentiment'].mutate(\n",
    "    CONSUMER_SENTIMENT  = fred_datasets['consumer-sentiment']['CONSUMER_SENTIMENT'].replace(',','').cast('float64'))\n",
    "\n",
    "for k,v in fred_datasets.items():\n",
    "    #print(k)\n",
    "    print(v.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Merge Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge all tables from the Investing.com dataset\n",
    "inv_data = prep.merge_tables(\n",
    "    inv_datasets, \n",
    "    join_key = 'DATE',\n",
    "    join_type = 'outer')\n",
    "\n",
    "# Merge all tables from the FRED dataset together\n",
    "fred_data = prep.merge_tables(\n",
    "    fred_datasets, \n",
    "    join_key = 'DATE', \n",
    "    join_type = 'outer')\n",
    "\n",
    "# Bring dates from inv table into FRED so values can be imputed\n",
    "fred_data = prep.merge_tables(\n",
    "    tables_to_merge={\n",
    "        'fred' : fred_data,\n",
    "        'inv' : inv_data.select('DATE')\n",
    "    },\n",
    "    join_key = 'DATE',\n",
    "    join_type = 'outer'\n",
    ")\n",
    "\n",
    "# Run the forward-fill imputation - fred data\n",
    "fred_data = prep.impute_forward_fill(\n",
    "    data = fred_data,\n",
    "    sort_by = 'DATE')\n",
    "\n",
    "# Filter out early nulls from FRED\n",
    "fred_data = fred_data.filter(fred_data.DATE.year() > 2006)\n",
    "\n",
    "# Run the forward-fill imputation - investing.com data\n",
    "# TODO decide whether to keep this\n",
    "inv_data = prep.impute_forward_fill(\n",
    "    data = inv_data,\n",
    "    sort_by = 'DATE')\n",
    "\n",
    "# Finally, merge the investing.com and FRED data\n",
    "# Using an inner join to exclude any days which aren't trading days\n",
    "data = prep.merge_tables(\n",
    "    {\n",
    "        'inv' : inv_data, \n",
    "        'fred' : fred_data,\n",
    "    },\n",
    "    join_key = 'DATE',\n",
    "    join_type='inner')\n",
    "\n",
    "# TODO For TRADING_DAY columns,  fill in NaNs with FALSE \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_pandas().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_pandas().isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Data Interpretability & Availability Corrections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Add REAL price columns by controlling for time-lagged CPI \n",
    "# (divide all prices by relative CPI, indexed to base year)\n",
    "# Applies to all commodities and equities indexes (ex VIX)\n",
    "# as well as hourly wages and median house prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Add a US_Housing_Starts_Per_Capita column \n",
    "# to provide a clearer economic health indicator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Offset macroeconomic indicators to align with when they're reported/available \n",
    "# or simplify by lagging all of them 3 months?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Feature Engineering for Forecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Add LAG columns - ~3 month, ~1 year, ~3 years?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Add XMA 10-day, 30-day, and 90-day columns to help with technical analysis?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Add RATIO columns looking at the relative prices of pairs of commodities?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dev-Holdout Data Split "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Starting each period with a crisis!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_end_year = 2019\n",
    "\n",
    "# Split the data into model development (2007 to 2019) \n",
    "dev_data = data.filter(data.DATE.year() >= 2007).filter(data.DATE.year() <= training_end_year)\n",
    "\n",
    "# and final holdout (2020 to late 2024)\n",
    "holdout_data = data.filter(data.DATE.year() > training_end_year)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Correlations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not especially useful since this is time-series data, but a quick look doesn't hurt.  Mostly "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO switch these to look at only economic indicators and \n",
    "# REAL prices, wages, and only closing prices\n",
    "dev_data.to_pandas().select_dtypes(include=np.number).corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Copper vs other commodities** - we see a mix of very tight positive correlations, in some cases almost perfectly linear.  Some positive correlation is to be expected since we haven't controlled for inflation yet, but inflation wasn't all that high during the dev period.  However, we also see surprisingly loose correlations in some cases, including separate price spikes.  There's some curious multimodality in both price distributions and price correlations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO switch these to REAL prices\n",
    "vis.plot_feature_correlation_matrix(\n",
    "    dev_data.select(\n",
    "        'copper_PRICE',\n",
    "        'oil-wti_PRICE',\n",
    "        'lumber_PRICE',\n",
    "        'corn_PRICE',\n",
    "        'natural-gas_PRICE',\n",
    "        'soy_PRICE',\n",
    "    ).to_pandas())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Copper vs economic indicators** - Also shows a mix of straightfoward linear correlation and curious drift and multimodality.  While some of the linear correlation can be attributed to inflation, much of it cannot be over this time frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Switch to REAL prices\n",
    "vis.plot_feature_correlation_matrix(\n",
    "    dev_data.select(\n",
    "        'copper_PRICE',\n",
    "        'CONSUMER_SENTIMENT',\n",
    "        'r2000_PRICE',\n",
    "        'UNEMPLOYMENT',\n",
    "        'HOUSE_STARTS'\n",
    "    ).to_pandas())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Plots & Preliminary Stationarity Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For forecasting, features should be stationary, meaning that no significant trends or seasonal patterns should be present in the data.  The mean and variance should be consistent throughout the time period."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are clearly not stationary.  The trends are enormous, and while seasonality is difficult to detect at this level, it's almost certainly present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "column_list = [\n",
    "    'DATE',\n",
    "    'copper_PRICE',\n",
    "    'CONSUMER_SENTIMENT',\n",
    "    'r2000_PRICE',\n",
    "    'UNEMPLOYMENT',\n",
    "    'HOUSE_STARTS'\n",
    "]\n",
    "df = dev_data.select(column_list).to_pandas()\n",
    "df['DATE'] = pd.to_datetime(df['DATE'])\n",
    "df = df.sort_values('DATE')\n",
    "fig, axes = vis.plot_pairwise_time_series_matrix(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many of these look pretty good at first differencing, but some definitely need to be differenced at least one more time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Develop visualization with 3-5 columns of time series charts \n",
    "# (original data, diff1, diff2, diff3...)\n",
    "# with one row of charts for each series "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take the first few differences to see how many iterations it'll take \n",
    "# to get to stationarity\n",
    "diff1 = dev_data.select(column_list).to_pandas()\n",
    "diff1['DATE'] = pd.to_datetime(diff1['DATE'])\n",
    "diff1 = diff1.sort_values('DATE')\n",
    "diff1[diff1.columns.drop('DATE')] = diff1[diff1.columns.drop('DATE')].diff().dropna()\n",
    "fig, axes = vis.plot_pairwise_time_series_matrix(diff1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Far fewwer concerns regarding stationarity at second differencing, but there are still some suspicious sections in some of these series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff2 = dev_data.select(column_list).to_pandas()\n",
    "diff2['DATE'] = pd.to_datetime(diff2['DATE'])\n",
    "diff2 = diff2.sort_values('DATE')\n",
    "diff2[diff2.columns.drop('DATE')] = diff2[diff2.columns.drop('DATE')].diff().diff().dropna()\n",
    "fig, axes = vis.plot_pairwise_time_series_matrix(diff2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dev_data.select(['DATE','copper_PRICE']).to_pandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['DATE'] = pd.to_numeric(pd.to_datetime(df['DATE']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Common Stationarity Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO add functional stationarity tests & interpretation\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.tsa.stattools import kpss\n",
    "adfuller(df['copper_PRICE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stationarity_tests = {\n",
    "    'adfuller' :  adfuller(df['copper_Price']),\n",
    "    'kpss' : kpss(df['copper_Price']) \n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Key tools:\n",
    "- [skforecast](https://skforecast.org/)\n",
    "- [pmdarima](https://github.com/alkaline-ml/pmdarima) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skforecast.sarimax import Sarimax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Pure ARIMA Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dev_data.to_pandas()\n",
    "df['DATE'] = pd.to_datetime(df['DATE'])\n",
    "pdq = (1,2,1) # p autoregression lags, d differences, q moving average\n",
    "model = Sarimax(order = pdq)\n",
    "model.fit(\n",
    "    y = df['copper_PRICE'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Simple ARIMAX Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using opening prices and volatility only to predict copper closing price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "exog_cols = [col for col in df.columns if '_OPEN' in col]\n",
    "exog = df[exog_cols]\n",
    "pdq = (1,2,1) # p autoregression lags, d differences, q moving average\n",
    "model = Sarimax(order = pdq)\n",
    "model.fit(\n",
    "    y = df['copper_PRICE'],\n",
    "    exog = exog)\n",
    "model.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
