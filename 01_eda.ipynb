{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 01 Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Designed for Python 3.11.9 on Windows 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install from requirements.txt\n",
    "!python -m pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative for running in Colab etc\n",
    "!pip install --upgrade pip\n",
    "!pip install numpy==1.24.1\n",
    "!pip install pmdarima==2.0.4\n",
    "!pip install ibis-framework[duckdb]==9.5.0\n",
    "!pip install pandas==2.2.3\n",
    "!pip install matplotlib==3.9.2\n",
    "!pip install skforecast==0.14.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and Preprocess Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Key tools:\n",
    "- [Ibis](https://ibis-project.org/) on a [DuckDB](https://duckdb.org/) backend with transformations to [Pandas](https://pandas.pydata.org/docs/index.html) for certain operations.\n",
    "- Some excessively-verbose functions spun out to a data prep module "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import ibis\n",
    "import matplotlib.pyplot as plt\n",
    "import skforecast\n",
    "import pmdarima\n",
    "import methods.prep as prep\n",
    "import methods.vis as vis\n",
    "import importlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(vis)\n",
    "importlib.reload(prep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load Individual Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con = ibis.connect(\"duckdb://\")\n",
    "data_path = 'data_eda/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load FRED datasets\n",
    "fred_dataset_names = [\n",
    "    'consumer-sentiment',\n",
    "    'cpi',\n",
    "    'hourly-wage',\n",
    "    'house-median-price',\n",
    "    'house-starts',\n",
    "    'unemployment',\n",
    "]\n",
    "\n",
    "fred_datasets = {}\n",
    "\n",
    "print('\\nFRED Datasets (single-variable time series)')\n",
    "for name in fred_dataset_names:\n",
    "    fred_datasets[name] = ibis.read_csv(\n",
    "        data_path + name + '.csv',\n",
    "        dateformat='%m/%d/%Y')\n",
    "    print(name + ': ' + str(fred_datasets[name].to_pandas().shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load investing.com datasets\n",
    "\n",
    "inv_dataset_names = [\n",
    "    'copper', \n",
    "    'corn', \n",
    "    'gold',\n",
    "    'lumber',\n",
    "    'natural-gas',\n",
    "    'oil-wti',\n",
    "    'r2000', \n",
    "    'soy',\n",
    "    'sp500',\n",
    "    'vix-volatility'\n",
    "]\n",
    "\n",
    "traded_commodities = [\n",
    "    'copper'\n",
    "]\n",
    "\n",
    "inv_datasets = {}\n",
    "\n",
    "print('Investing.com Datasets (standard prices and volume time series)')\n",
    "for name in inv_dataset_names:\n",
    "    inv_datasets[name] = ibis.read_csv(\n",
    "        data_path + name + '.csv',\n",
    "        dateformat='%m/%d/%Y')\n",
    "    print(name + ': ' + str(inv_datasets[name].to_pandas().shape))\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Initial Preprocessing - Individual Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in inv_datasets.keys():\n",
    "    #Drop unnecessary columns\n",
    "    inv_datasets[key] = inv_datasets[key].drop('Vol.','Change %')\n",
    "\n",
    "    #Add trading day flag to traded columns\n",
    "    if key in traded_commodities:\n",
    "        inv_datasets[key] = inv_datasets[key].mutate(\n",
    "            TRADING_DAY = True\n",
    "        )\n",
    "\n",
    "    #Convert column names to ALL CAPS\n",
    "    inv_datasets[key] = inv_datasets[key].rename('ALL_CAPS')\n",
    "\n",
    "    # Convert PRICE, OPEN, HIGH, and LOW to float64s if they are strings\n",
    "    if inv_datasets[key]['PRICE'].type().is_string():\n",
    "        inv_datasets[key] = inv_datasets[key].mutate(\n",
    "            PRICE = inv_datasets[key]['PRICE'].replace(',','').cast('float64'))    \n",
    "\n",
    "    if inv_datasets[key]['OPEN'].type().is_string():\n",
    "        inv_datasets[key] = inv_datasets[key].mutate(\n",
    "            OPEN = inv_datasets[key]['OPEN'].replace(',','').cast('float64'))   \n",
    "\n",
    "    if inv_datasets[key]['HIGH'].type().is_string():\n",
    "        inv_datasets[key] = inv_datasets[key].mutate(\n",
    "            HIGH = inv_datasets[key]['HIGH'].replace(',','').cast('float64'))  \n",
    "\n",
    "    if inv_datasets[key]['LOW'].type().is_string():\n",
    "        inv_datasets[key] = inv_datasets[key].mutate(\n",
    "            LOW = inv_datasets[key]['LOW'].replace(',','').cast('float64'))   \n",
    "\n",
    "    # Add prefixes to each column based on the name of the dataset\n",
    "    names_map = {\n",
    "        f\"{key}_{col}\" : col \n",
    "            for col in inv_datasets[key].columns\n",
    "            if col != 'DATE'}\n",
    "    inv_datasets[key] = inv_datasets[key].rename(names_map)\n",
    "\n",
    "for k,v in inv_datasets.items():\n",
    "    print(v.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix column names in FRED data and drop unused rows\n",
    "for key in fred_datasets.keys():\n",
    "    #Rename second column (data) to name of dataset\n",
    "    old_col_name = fred_datasets[key].columns[1]\n",
    "    new_col_name = key\n",
    "    fred_datasets[key] = fred_datasets[key].rename({new_col_name: old_col_name})\n",
    "\n",
    "    #Convert timestamps to dates\n",
    "    fred_datasets[key] = fred_datasets[key].mutate(\n",
    "        DATE = fred_datasets[key]['DATE'].cast('date'))\n",
    "\n",
    "    #Convert to snake-case\n",
    "    fred_datasets[key] = fred_datasets[key].rename('ALL_CAPS')\n",
    "\n",
    "    #Drop unused rows - but keep some in 2006 to support forward fill\n",
    "    fred_datasets[key] = fred_datasets[key].filter(fred_datasets[key].DATE.year() > 2005)\n",
    "\n",
    "# Convert string to float64 in consumer sentiment data   \n",
    "fred_datasets['consumer-sentiment'] = fred_datasets['consumer-sentiment'].mutate(\n",
    "    CONSUMER_SENTIMENT  = fred_datasets['consumer-sentiment']['CONSUMER_SENTIMENT'].replace(',','').cast('float64'))\n",
    "\n",
    "for k,v in fred_datasets.items():\n",
    "    #print(k)\n",
    "    print(v.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Merge Datasets & Forward Fill Missing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the datasets all cover the desired 2007-2024 date range, the specific dates covered by each dataset vary widely. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Merge all tables from the Investing.com dataset\n",
    "# inv_data = prep.merge_tables(\n",
    "#     inv_datasets, \n",
    "#     join_key = 'DATE',\n",
    "#     join_type = 'outer')\n",
    "\n",
    "# # Merge all tables from the FRED dataset together\n",
    "# fred_data = prep.merge_tables(\n",
    "#     fred_datasets, \n",
    "#     join_key = 'DATE', \n",
    "#     join_type = 'outer')\n",
    "\n",
    "# # Bring dates from inv table into FRED so values can be imputed\n",
    "# fred_data = prep.merge_tables(\n",
    "#     tables_to_merge={\n",
    "#         'fred' : fred_data,\n",
    "#         'inv' : inv_data.select('DATE')\n",
    "#     },\n",
    "#     join_key = 'DATE',\n",
    "#     join_type = 'outer'\n",
    "# )\n",
    "\n",
    "# # Run the forward-fill imputation - fred data\n",
    "# fred_data = prep.impute_forward_fill(\n",
    "#     data = fred_data,\n",
    "#     sort_by = 'DATE')\n",
    "\n",
    "# # Filter out early nulls from FRED\n",
    "# fred_data = fred_data.filter(fred_data.DATE.year() > 2006)\n",
    "\n",
    "# # Run the forward-fill imputation - investing.com data\n",
    "# # TODO decide whether to keep this\n",
    "# inv_data = prep.impute_forward_fill(\n",
    "#     data = inv_data,\n",
    "#     sort_by = 'DATE')\n",
    "\n",
    "# # Finally, merge the investing.com and FRED data\n",
    "# # Using an inner join to exclude any days which aren't trading days\n",
    "# data = prep.merge_tables(\n",
    "#     {\n",
    "#         'inv' : inv_data, \n",
    "#         'fred' : fred_data,\n",
    "#     },\n",
    "#     join_key = 'DATE',\n",
    "#     join_type='inner')\n",
    "\n",
    "# # TODO For TRADING_DAY columns,  fill in NaNs with FALSE \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "# Merge all tables from the Investing.com dataset\n",
    "inv_data = prep.merge_tables(\n",
    "    inv_datasets, \n",
    "    join_key = 'DATE',\n",
    "    join_type = 'outer')\n",
    "\n",
    "# Merge all tables from the FRED dataset together\n",
    "fred_data = prep.merge_tables(\n",
    "    fred_datasets, \n",
    "    join_key = 'DATE', \n",
    "    join_type = 'outer')\n",
    "\n",
    "# Filter out early nulls from FRED\n",
    "fred_data = fred_data.filter(fred_data.DATE.year() > 2006)\n",
    "\n",
    "# Create a table with all of the dates in range\n",
    "daterange = []\n",
    "current_date = datetime.date(2006,1,1)\n",
    "while current_date <= datetime.date(2024, 10, 31):\n",
    "    daterange.append(current_date)\n",
    "    current_date = current_date + datetime.timedelta(days=1)\n",
    "daterange = pd.DataFrame(daterange, columns=['DATE'])\n",
    "\n",
    "# Finally, merge the investing.com, FRED data, and daterange\n",
    "# using an outer join to ensure there is a row for every date\n",
    "data = prep.merge_tables(\n",
    "    {\n",
    "        'inv' : inv_data, \n",
    "        'fred' : fred_data,\n",
    "        'dates': daterange\n",
    "    },\n",
    "    join_key = 'DATE',\n",
    "    join_type='outer')\n",
    "\n",
    "# Forward-fill impute all numeric data \n",
    "data = prep.impute_forward_fill_numerics(\n",
    "    data = data,\n",
    "    sort_by = 'DATE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data.filter(data.DATE.year() >= 2007).filter(data.DATE.year() <= 2019).to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_pandas().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Data Comparability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparisons over time can be improved by calculating real (inflation-corrected) prices and adjusting housing starts to a per-capita rate.  This will also take some of the trends out of the data, improving stationarity for forecasting purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Add REAL price columns by controlling for time-lagged CPI \n",
    "# (divide all prices by relative CPI, indexed to base year)\n",
    "# Applies to all commodities and equities indexes (ex VIX)\n",
    "# as well as hourly wages and median house prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Add a US_Housing_Starts_Per_Capita column \n",
    "# TODO would need to bring in population indicator\n",
    "# to provide a clearer economic health indicator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Data Availability Corrections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data that is not yet available at prediction time can't be used to make predictions.  Many macroeconomic indicators are unavailable for weeks or months after the dates they pertain to as the underlying data must be reported and processed.  At this point, we'll avoid this concern by lagging all macroeconomic indicators by 90 days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Offset macroeconomic indicators to align with when they're reported/available \n",
    "# or simplify by lagging all of them 3 months?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Feature Engineering for Forecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Add LAG columns - ~3 month, ~1 year, ~3 years?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Add XMA 10-day, 30-day, and 90-day columns to help with technical analysis?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dev-Holdout Data Split "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Starting each period with a crisis!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_end_year = 2019\n",
    "\n",
    "# Split the data into model development (2007-01-01 to 2019-12-31) \n",
    "dev_data = data.filter(data.DATE.year() >= 2007).filter(data.DATE.year() <= training_end_year)\n",
    "\n",
    "# and final holdout (2020-01-01 to 2024-10-31)\n",
    "holdout_data = data.filter(data.DATE.year() > training_end_year)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Correlations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not especially useful since this is time-series data, but a quick look doesn't hurt.  Mostly "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO switch these to look at only economic indicators and \n",
    "# REAL prices, wages, and only closing prices\n",
    "dev_data.to_pandas().select_dtypes(include=np.number).corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Copper vs other commodities** - we see a mix of very tight positive correlations, in some cases almost perfectly linear.  Some positive correlation is to be expected since we haven't controlled for inflation yet, but inflation wasn't all that high during the dev period.  However, we also see surprisingly loose correlations in some cases, including separate price spikes.  There's some curious multimodality in both price distributions and price correlations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO switch these to REAL prices\n",
    "vis.plot_feature_correlation_matrix(\n",
    "    dev_data.select(\n",
    "        'copper_PRICE',\n",
    "        'oil-wti_PRICE',\n",
    "        'lumber_PRICE',\n",
    "        'corn_PRICE',\n",
    "        'natural-gas_PRICE',\n",
    "        'soy_PRICE',\n",
    "    ).to_pandas())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Copper vs economic indicators** - Also shows a mix of straightfoward linear correlation and curious drift and multimodality.  While some of the linear correlation can be attributed to inflation, much of it cannot be over this time frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Switch to REAL prices\n",
    "vis.plot_feature_correlation_matrix(\n",
    "    dev_data.select(\n",
    "        'copper_PRICE',\n",
    "        'CONSUMER_SENTIMENT',\n",
    "        'r2000_PRICE',\n",
    "        'UNEMPLOYMENT',\n",
    "        'HOUSE_STARTS'\n",
    "    ).to_pandas())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Plots & Preliminary Stationarity Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For forecasting, features should be stationary, meaning that no significant trends or seasonal patterns should be present in the data.  The mean and variance should be consistent throughout the time period.\n",
    "\n",
    "No features are stationary without differencing.  The trends are enormous, and while seasonality is difficult to detect at this level, it's almost certainly present.  Many of these look pretty good at first differencing, but some definitely need to be differenced at least one more time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "column_list = [\n",
    "    'DATE',\n",
    "    'copper_PRICE',\n",
    "    'CONSUMER_SENTIMENT',\n",
    "    'r2000_PRICE',\n",
    "    'UNEMPLOYMENT',\n",
    "    'HOUSE_STARTS'\n",
    "]\n",
    "df = dev_data.select(column_list).to_pandas()\n",
    "df['DATE'] = pd.to_datetime(df['DATE'])\n",
    "df = df.sort_values('DATE')\n",
    "fig, axes = vis.plot_time_series_diffs(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Common Stationarity Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO add functional stationarity tests & interpretation\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.tsa.stattools import kpss\n",
    "adfuller(df['copper_PRICE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stationarity_tests = {\n",
    "    'adfuller' :  adfuller(df['copper_Price']),\n",
    "    'kpss' : kpss(df['copper_Price']) \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Key tools:\n",
    "- [skforecast](https://skforecast.org/)\n",
    "- [pmdarima](https://github.com/alkaline-ml/pmdarima)\n",
    "- [sklearn scaling]()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skforecast.sarimax import Sarimax\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Remove Nontrading Days"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, all required lags and moving-average values have been calculated.  For calculating deltas, we will only want trading days. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Pure ARIMA Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dev_data.to_pandas()\n",
    "df['DATE'] = pd.to_datetime(df['DATE'])\n",
    "pdq = (1,1,1) # p autoregression lags, d differences, q moving average\n",
    "model = Sarimax(order = pdq)\n",
    "model.fit(\n",
    "    y = df['copper_PRICE'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Simple ARIMAX Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using opening prices and volatility only to predict copper closing price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dev_data.to_pandas()\n",
    "exog_cols = [col for col in df.columns if '_OPEN' in col]\n",
    "exog = df[exog_cols]\n",
    "pdq = (1,1,1) # p autoregression lags, d differences, q moving average\n",
    "model = Sarimax(order = pdq)\n",
    "model.fit(\n",
    "    y = df['copper_PRICE'],\n",
    "    exog = exog)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dev_data.to_pandas()\n",
    "(df['copper_OPEN'] - df['copper_OPEN'].mean())/df['copper_OPEN'].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dev_data.to_pandas()\n",
    "exog_cols = [col for col in df.columns if '_OPEN' in col]\n",
    "exog = df[exog_cols]\n",
    "exog = exog.drop(['natural-gas_OPEN','gold_OPEN','corn_OPEN'], axis='columns')\n",
    "exog = (exog - exog.mean())/exog.std()\n",
    "\n",
    "target = df['copper_PRICE']\n",
    "target = (target - target.mean())/target.std()\n",
    "\n",
    "pdq = (1,1,1) # p autoregression lags, d differences, q moving average\n",
    "model = Sarimax(order = pdq)\n",
    "model.fit(\n",
    "    y = target,\n",
    "    exog = exog)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
